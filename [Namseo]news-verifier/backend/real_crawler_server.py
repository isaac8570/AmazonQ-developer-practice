#!/usr/bin/env python3

print("Starting Real Crawling FastAPI server...")

try:
    from fastapi import FastAPI
    from fastapi.middleware.cors import CORSMiddleware
    import uvicorn
    import requests
    from bs4 import BeautifulSoup
    import urllib.parse
    from datetime import datetime
    import re
    
    print("All imports successful!")
    
    app = FastAPI()
    
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # í¬ë¡¤ë§ í´ëž˜ìŠ¤
    class NewsCrawler:
        def __init__(self):
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
        
        def crawl_naver_news(self, query):
            articles = []
            try:
                encoded_query = urllib.parse.quote(query)
                url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}&sort=1"
                
                print(f"ðŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§: {url}")
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                news_items = soup.select('.news_area')
                
                print(f"ðŸ“° ë„¤ì´ë²„ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                for i, item in enumerate(news_items[:8]):
                    try:
                        title_elem = item.select_one('.news_tit')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        url_link = title_elem.get('href', '')
                        
                        # ë„ë©”ì¸ ì¶”ì¶œ
                        try:
                            domain = urllib.parse.urlparse(url_link).netloc
                        except:
                            domain = 'naver.com'
                        
                        # ìš”ì•½ í…ìŠ¤íŠ¸
                        summary_elem = item.select_one('.news_dsc')
                        summary = summary_elem.get_text(strip=True) if summary_elem else title
                        
                        # ì‹œê°„ ì •ë³´
                        time_elem = item.select_one('.info_group .info')
                        time_text = time_elem.get_text(strip=True) if time_elem else ""
                        
                        # ì‹ ë¢°ë„ ê³„ì‚°
                        confidence = self.calculate_confidence(domain)
                        
                        articles.append({
                            'id': f"naver_{i}",
                            'title': title,
                            'content': summary,
                            'url': url_link,
                            'domain': domain,
                            'source_type': 'press',
                            'timestamp': self.parse_time(time_text),
                            'confidence': confidence
                        })
                        
                    except Exception as e:
                        print(f"âŒ ë„¤ì´ë²„ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                        
            except Exception as e:
                print(f"âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
            return articles
        
        def crawl_daum_news(self, query):
            articles = []
            try:
                encoded_query = urllib.parse.quote(query)
                url = f"https://search.daum.net/search?w=news&q={encoded_query}&sort=recency"
                
                print(f"ðŸ” ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§: {url}")
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                news_items = soup.select('.item-news')
                
                print(f"ðŸ“° ë‹¤ìŒì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                for i, item in enumerate(news_items[:5]):
                    try:
                        title_elem = item.select_one('.tit-news a')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        url_link = title_elem.get('href', '')
                        
                        try:
                            domain = urllib.parse.urlparse(url_link).netloc
                        except:
                            domain = 'daum.net'
                        
                        summary_elem = item.select_one('.desc')
                        summary = summary_elem.get_text(strip=True) if summary_elem else title
                        
                        time_elem = item.select_one('.date')
                        time_text = time_elem.get_text(strip=True) if time_elem else ""
                        
                        confidence = self.calculate_confidence(domain)
                        
                        articles.append({
                            'id': f"daum_{i}",
                            'title': title,
                            'content': summary,
                            'url': url_link,
                            'domain': domain,
                            'source_type': 'press',
                            'timestamp': self.parse_time(time_text),
                            'confidence': confidence
                        })
                        
                    except Exception as e:
                        print(f"âŒ ë‹¤ìŒ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                        
            except Exception as e:
                print(f"âŒ ë‹¤ìŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
            return articles
        
        def calculate_confidence(self, domain):
            high_trust = [
                'yonhapnews.co.kr', 'yna.co.kr', 'kbs.co.kr', 'mbc.co.kr', 'sbs.co.kr',
                'chosun.com', 'donga.com', 'joongang.co.kr', 'hani.co.kr', 'khan.co.kr'
            ]
            medium_trust = [
                'naver.com', 'daum.net', 'mk.co.kr', 'mt.co.kr', 'etnews.com'
            ]
            
            if any(trusted in domain for trusted in high_trust):
                return 'High'
            elif any(trusted in domain for trusted in medium_trust):
                return 'Mid'
            else:
                return 'Low'
        
        def parse_time(self, time_text):
            try:
                now = datetime.now()
                
                if 'ë¶„ ì „' in time_text:
                    minutes = int(re.findall(r'(\d+)ë¶„ ì „', time_text)[0])
                    time_obj = now.replace(minute=max(0, now.minute - minutes))
                elif 'ì‹œê°„ ì „' in time_text:
                    hours = int(re.findall(r'(\d+)ì‹œê°„ ì „', time_text)[0])
                    time_obj = now.replace(hour=max(0, now.hour - hours))
                elif 'ì¼ ì „' in time_text:
                    days = int(re.findall(r'(\d+)ì¼ ì „', time_text)[0])
                    time_obj = now.replace(day=max(1, now.day - days))
                else:
                    time_obj = now
                    
                return time_obj.isoformat()
            except:
                return datetime.now().isoformat()
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
    crawler = NewsCrawler()
    
    @app.get("/")
    def root():
        return {"message": "Real Crawling Backend is working!", "status": "ok"}
    
    @app.get("/trending")
    def trending():
        return {
            "topics": [
                {"id": 1, "title": "ì‚¼ì„±ì „ìž", "count": 15, "trend": "up", "category": "tech"},
                {"id": 2, "title": "ì•„ì´í°", "count": 12, "trend": "up", "category": "tech"},
                {"id": 3, "title": "ì •ì¹˜", "count": 8, "trend": "stable", "category": "politics"}
            ]
        }
    
    @app.post("/search")
    def search(request: dict):
        query = request.get("query", "")
        if not query:
            return {"clusters": []}
        
        print(f"ðŸ” ê²€ìƒ‰ ìš”ì²­: {query}")
        
        # ì‹¤ì œ í¬ë¡¤ë§ ìˆ˜í–‰
        all_articles = []
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
        naver_articles = crawler.crawl_naver_news(query)
        all_articles.extend(naver_articles)
        
        # ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§
        daum_articles = crawler.crawl_daum_news(query)
        all_articles.extend(daum_articles)
        
        print(f"âœ… ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")
        
        if not all_articles:
            return {"clusters": []}
        
        # ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ (ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜)
        clusters = []
        used_articles = set()
        
        for i, article in enumerate(all_articles):
            if i in used_articles:
                continue
                
            cluster_articles = [article]
            used_articles.add(i)
            
            # ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ ì°¾ê¸°
            for j, other_article in enumerate(all_articles[i+1:], i+1):
                if j in used_articles:
                    continue
                    
                # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
                words1 = set(article['title'].lower().split())
                words2 = set(other_article['title'].lower().split())
                
                if words1 and words2:
                    intersection = words1.intersection(words2)
                    union = words1.union(words2)
                    similarity = len(intersection) / len(union) if union else 0.0
                    
                    if similarity > 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•˜ë©´ ê°™ì€ í´ëŸ¬ìŠ¤í„°
                        cluster_articles.append(other_article)
                        used_articles.add(j)
            
            # í´ëŸ¬ìŠ¤í„° ìƒì„±
            cluster_label = cluster_articles[0]['title']
            if len(cluster_label) > 50:
                cluster_label = cluster_label[:47] + "..."
            
            cluster = {
                'id': f'cluster_{len(clusters)}',
                'label': cluster_label,
                'size': len(cluster_articles),
                'articles': cluster_articles,
                'trace_score': sum(0.8 if a['confidence'] == 'High' else 0.6 if a['confidence'] == 'Mid' else 0.4 for a in cluster_articles) / len(cluster_articles)
            }
            clusters.append(cluster)
        
        return {"clusters": clusters}
    
    print("ðŸš€ Starting real crawling server on http://localhost:8080")
    uvicorn.run(app, host="0.0.0.0", port=8080)
    
except ImportError as e:
    print(f"Import error: {e}")
    input("Press Enter to exit...")
except Exception as e:
    print(f"Error: {e}")
    input("Press Enter to exit...")